# ðŸ§  Aika Memory Architecture v2 â€” Design Document

> **Status:** Design phase â€” not yet implemented.
> **Goal:** Replace Aika's flat 3-tier memory with a brain-inspired system that maximizes awareness (knowledge density per context token) while staying practical on a Raspberry Pi 5.

---

## 1. What's Wrong with the Current System

The current memory has three tiers: **Buffer** (10 raw messages) â†’ **Weekly Summary** (3 sentences) â†’ **Global Summary** (4 sentences). This has fundamental problems:

| Problem | Why it matters |
|---------|---------------|
| **No selective recall** | Every turn dumps the same last 10 messages + 2 summaries into the prompt, regardless of what's being discussed. Aika can't "remember" something from 3 weeks ago even if it's directly relevant. |
| **Lossy compression is one-way** | Once messages are summarized into the weekly summary, the original detail is gone forever. A 3-sentence summary can't capture "you mentioned wanting to buy a keyboard on Feb 3rd." |
| **No associations** | Memories have no links to each other. There's no way for a keyword or topic to trigger recall of a related past event â€” the "scent â†’ story" effect doesn't exist. |
| **No concept of importance** | A casual "lol" and a critical "remember: my server password is X" are treated identically. Both get the same buffer slot and the same summarization treatment. |
| **Time-based expiry is too aggressive** | Messages older than 1 hour are force-expired regardless of whether they contained important information. A deeply important conversation at 2 PM is gone by 3 PM. |
| **Context is wasted** | The global + weekly summaries are always injected, even when irrelevant. They consume tokens without adding value on most turns. |

**The core issue:** Storage and context are conflated. The buffer IS the context. There's no retrieval â€” just a fixed window.

---

## 2. Design Principles

1. **Separate storage from context.** Store everything. Retrieve selectively. Context is assembled per-turn by a "composer" that acts like attention.
2. **Memories are nodes, not a log.** Instead of a message timeline, we store discrete memory nodes â€” facts, events, preferences, procedures â€” each with metadata.
3. **Associations are first-class.** Links between memories have weights that strengthen with use and decay with time. This enables pattern-completion retrieval.
4. **Strength gates retrieval, not deletion.** Don't delete weak memories â€” just make them harder to retrieve. Strong memories surface easily; weak ones require exact cues.
5. **Budget is sacred.** Every token in the prompt must earn its place. The Context Composer enforces a hard token budget with priority allocation.
6. **Decay is healthy.** Forgetting irrelevant associations is a feature, not a bug. It keeps the mind clean and retrieval fast.

---

## 3. Architecture Overview

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚       CONTEXT COMPOSER        â”‚
                        â”‚   (Budgeted Prompt Assembly)  â”‚
                        â”‚                              â”‚
                        â”‚  Token Budget Allocation:    â”‚
                        â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”       â”‚
                        â”‚  â”‚40% â”‚ â”‚40% â”‚ â”‚20% â”‚       â”‚
                        â”‚  â”‚Taskâ”‚ â”‚Sem.â”‚ â”‚Epi.â”‚       â”‚
                        â”‚  â””â”€â”€â”¬â”€â”˜ â””â”€â”€â”¬â”€â”˜ â””â”€â”€â”¬â”€â”˜       â”‚
                        â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚      â”‚      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                â–¼                  â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ WORKING  â”‚    â”‚   SEMANTIC    â”‚    â”‚  EPISODIC  â”‚
            â”‚   SET    â”‚    â”‚    STORE      â”‚    â”‚   STORE    â”‚
            â”‚          â”‚    â”‚              â”‚    â”‚            â”‚
            â”‚ Current  â”‚    â”‚ Stable facts â”‚    â”‚ Timestampedâ”‚
            â”‚ turns +  â”‚    â”‚ preferences  â”‚    â”‚  "scenes"  â”‚
            â”‚ task     â”‚    â”‚ definitions  â”‚    â”‚  events    â”‚
            â”‚ state    â”‚    â”‚ commitments  â”‚    â”‚  moments   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚                  â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  PROCEDURAL      â”‚
                                    â”‚  STORE            â”‚
                                    â”‚                  â”‚
                                    â”‚ "How I do things"â”‚
                                    â”‚ Format prefs     â”‚
                                    â”‚ Tool habits      â”‚
                                    â”‚ Routines         â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚      ASSOCIATION INDEX       â”‚
                              â”‚                              â”‚
                              â”‚  Keyword/entity cue index    â”‚
                              â”‚  Weighted edges between      â”‚
                              â”‚  memory nodes                â”‚
                              â”‚  Decay + reinforcement       â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Memory Node Types

Every memory is a **node** with a type, metadata, and a strength score.

### 4.1 Episodic Memory (EM)
**What:** Time-stamped "scenes" â€” chunks of conversation or events that happened.
**Examples:**
- "On Feb 5, Erden asked me to set up a cron job for backups."
- "At 2:30 AM, Erden was debugging SSL certificates and was frustrated."

**Properties:**
- Has a specific timestamp and duration
- Contains emotional coloring / context
- High detail, medium lifespan
- Useful when exact history matters ("what did we do last Tuesday?")

### 4.2 Semantic Memory (SM)
**What:** Distilled, stable facts extracted from episodes. The "truths" that persist.
**Examples:**
- "Erden's timezone is America/Los_Angeles."
- "Erden prefers direct, no-fluff responses."
- "The server runs on Raspberry Pi 5 at home."
- "Erden is working on a car wash business website."

**Properties:**
- No specific timestamp (timeless truths)
- High confidence, long lifespan
- Updated/corrected when contradicted by new information
- Most cost-effective memory type (highest value per token)

### 4.3 Procedural Memory (PM)
**What:** "How I do things" â€” learned patterns, format preferences, routines.
**Examples:**
- "When Erden asks to check server health, run: `htop`, `df -h`, `free -m`."
- "Erden prefers code without excessive comments."
- "For deployment, always check systemd service status first."

**Properties:**
- Behaviorally relevant â€” affects HOW Aika responds, not WHAT she knows
- Rarely needs to be shown in context (influences system prompt instead)
- Very stable, rarely decays

### 4.4 Working Set (WS)
**What:** The immediate conversation buffer. Last ~10-20 messages of the active conversation, plus any active task state (e.g., "currently diagnosing a networking issue").
**Not stored as nodes** â€” this is just the raw recent messages, similar to current buffer, but smarter about what counts as "active conversation" (uses time-gap detection).

---

## 5. Data Model (SQLite)

### Table: `memory_nodes`
```sql
CREATE TABLE memory_nodes (
    id          INTEGER PRIMARY KEY AUTOINCREMENT,
    type        TEXT NOT NULL,            -- 'episodic', 'semantic', 'procedural'
    title       TEXT NOT NULL,            -- 1-line summary for quick scanning
    content     TEXT NOT NULL,            -- Full content (2-5 sentences max)
    strength    REAL NOT NULL DEFAULT 1.0,-- Decays over time, reinforced on use
    created_at  REAL NOT NULL,           -- Unix timestamp
    last_accessed REAL NOT NULL,         -- Last time this was retrieved into context
    access_count INTEGER DEFAULT 0,      -- How many times retrieved
    source_turn INTEGER,                 -- Which conversation turn created this
    metadata    TEXT                      -- JSON: entities, tags, emotional_weight, etc.
);
```

### Table: `memory_edges`
```sql
CREATE TABLE memory_edges (
    id          INTEGER PRIMARY KEY AUTOINCREMENT,
    source_id   INTEGER NOT NULL REFERENCES memory_nodes(id) ON DELETE CASCADE,
    target_id   INTEGER NOT NULL REFERENCES memory_nodes(id) ON DELETE CASCADE,
    weight      REAL NOT NULL DEFAULT 1.0,  -- Strengthens with co-retrieval, decays over time
    relation    TEXT,                        -- Optional: 'caused_by', 'related_to', 'contradicts', 'refines'
    created_at  REAL NOT NULL,
    last_used   REAL NOT NULL,
    UNIQUE(source_id, target_id)
);
```

### Table: `memory_cues`
```sql
CREATE TABLE memory_cues (
    id          INTEGER PRIMARY KEY AUTOINCREMENT,
    node_id     INTEGER NOT NULL REFERENCES memory_nodes(id) ON DELETE CASCADE,
    cue_type    TEXT NOT NULL,              -- 'keyword', 'entity', 'tag', 'trigger_phrase'
    cue_value   TEXT NOT NULL               -- The actual cue text (lowercased)
);

CREATE INDEX idx_cues_value ON memory_cues(cue_value);
CREATE INDEX idx_cues_node ON memory_cues(node_id);
```

### Table: `messages` (Working Set â€” similar to current)
```sql
CREATE TABLE messages (
    id        INTEGER PRIMARY KEY AUTOINCREMENT,
    role      TEXT NOT NULL,
    content   TEXT NOT NULL,
    timestamp REAL NOT NULL
);
```

### Table: `memory_blobs` (Optional â€” raw evidence archive)
```sql
CREATE TABLE memory_blobs (
    id          INTEGER PRIMARY KEY AUTOINCREMENT,
    node_id     INTEGER REFERENCES memory_nodes(id) ON DELETE SET NULL,
    raw_text    TEXT NOT NULL,              -- Original transcript chunk
    timestamp   REAL NOT NULL
);
```

---

## 6. Core Pipelines

### 6.1 Ingest Pipeline (runs after every turn, in background)

```
New turn complete (user message + model response)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Store in Working Set           â”‚
â”‚  (Same as current buffer insert)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Extract Memories (via Groq)    â”‚
â”‚                                         â”‚
â”‚  Prompt Groq to extract from this turn: â”‚
â”‚  - Any new FACTS (â†’ semantic)           â”‚
â”‚  - Any EVENT worth remembering (â†’ epi.) â”‚
â”‚  - Any BEHAVIOR PATTERN (â†’ procedural)  â”‚
â”‚  - Cue keywords for each                â”‚
â”‚  - Importance score (1-5)               â”‚
â”‚                                         â”‚
â”‚  Output: structured JSON                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Deduplicate & Merge            â”‚
â”‚                                         â”‚
â”‚  Check if extracted fact already exists: â”‚
â”‚  - Match by cues/keywords               â”‚
â”‚  - If match: UPDATE existing node       â”‚
â”‚    (reinforce strength, merge content)  â”‚
â”‚  - If new: INSERT new node              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Link                           â”‚
â”‚                                         â”‚
â”‚  Connect new/updated nodes to existing  â”‚
â”‚  nodes that share entities or keywords  â”‚
â”‚  (INSERT into memory_edges)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Critical:** Steps 2-4 use **Groq** (fast, cheap, `qwen3-32b`) â€” NOT Gemini. This keeps costs near zero. The extraction prompt would look like:

```
Given this conversation turn, extract any memories worth storing.
Return JSON array. Each item has:
- type: "semantic" | "episodic" | "procedural"
- title: 1-line summary
- content: 2-4 sentences of detail
- importance: 1-5 (5 = critical, 1 = trivial)
- keywords: list of 3-8 cue words
- entities: list of named entities (people, places, projects)

Turn:
[user]: {user_message}
[model]: {model_response}

Rules:
- Only extract if there's something genuinely worth remembering.
- Most casual turns produce 0 memories. That's fine.
- Semantic: stable facts, preferences, commitments.
- Episodic: notable events, decisions, emotional moments.
- Procedural: learned patterns about how the user wants things done.
- Return empty array [] if nothing worth storing.
```

### 6.2 Retrieval Pipeline (runs at the START of every turn, before Gemini)

```
User sends message
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Extract Query Cues             â”‚
â”‚                                         â”‚
â”‚  From the user's message, extract:      â”‚
â”‚  - Keywords (simple tokenization)       â”‚
â”‚  - Named entities                       â”‚
â”‚  - Intent category                      â”‚
â”‚  (This can be rule-based, no LLM needed)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Candidate Pull (Cheap Pass)    â”‚
â”‚                                         â”‚
â”‚  Query memory_cues table for matches    â”‚
â”‚  â†’ Get candidate node IDs              â”‚
â”‚  Score: cue_matches + strength          â”‚
â”‚  â†’ Top 20-30 candidates                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Spreading Activation           â”‚
â”‚  (1-2 hops on memory_edges)             â”‚
â”‚                                         â”‚
â”‚  From top candidates, follow edges:     â”‚
â”‚  - Hop 1: neighbors with weight > 0.3   â”‚
â”‚  - Hop 2 (optional): if few candidates  â”‚
â”‚  Add connected nodes to candidate pool  â”‚
â”‚  This is the "scent â†’ story" mechanism  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Rank & Select                  â”‚
â”‚                                         â”‚
â”‚  Final score per candidate:             â”‚
â”‚  score = cue_match_score                â”‚
â”‚        + 0.6 * strength                 â”‚
â”‚        + 0.3 * recency_boost            â”‚
â”‚        - redundancy_penalty             â”‚
â”‚                                         â”‚
â”‚  Select top 6-12 nodes for context      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Reinforce                      â”‚
â”‚                                         â”‚
â”‚  Every node that made it into context:  â”‚
â”‚  - strength += 0.1                      â”‚
â”‚  - last_accessed = now                  â”‚
â”‚  - access_count += 1                    â”‚
â”‚  Every edge between co-retrieved nodes: â”‚
â”‚  - weight += 0.05                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key insight:** Step 2 (candidate pull) is pure SQLite keyword matching â€” no embeddings needed, no LLM call. This is the "cheap pass" that handles 90% of retrieval. We can add embedding-based search as an enhancement later, but keyword + entity matching on the `memory_cues` table is fast, free, and surprisingly effective for a single-user system.

### 6.3 Decay Pipeline (runs periodically â€” e.g., every hour or on each turn)

```sql
-- Decay all node strengths (half-life based)
-- Nodes with high access_count decay slower
UPDATE memory_nodes
SET strength = strength * POWER(0.995, ((:now - last_accessed) / 3600.0) / (1 + LOG(1 + access_count)))
WHERE strength > 0.01;

-- Decay all edge weights
UPDATE memory_edges
SET weight = weight * POWER(0.99, (:now - last_used) / 3600.0)
WHERE weight > 0.01;

-- Archive dead nodes (strength < 0.05, not accessed in 30+ days)
-- Don't delete â€” just mark as archived so they can be resurrected if directly queried
UPDATE memory_nodes
SET type = 'archived_' || type
WHERE strength < 0.05
  AND last_accessed < :now - (30 * 86400)
  AND type NOT LIKE 'archived_%';

-- Delete truly dead edges
DELETE FROM memory_edges WHERE weight < 0.01;
```

**Edges decay FASTER than nodes.** This is critical. A memory can survive indefinitely if it's strong, but its associations weaken unless reinforced. This means:
- Old strong memories become "isolated" over time â€” retrievable only by direct cue match, not by association
- Frequently co-activated memories form strong clusters
- The association graph stays clean and fast

---

## 7. Context Composer

The Context Composer replaces the current "dump everything" approach. It runs before each Gemini API call and assembles the prompt within a **strict token budget**.

### Token Budget Allocation
```
Total context budget: ~2000 tokens (adjustable)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ZONE 1 â€” Task State (40%, ~800 tokens)            â”‚
â”‚  â”œâ”€ Current user message                           â”‚
â”‚  â”œâ”€ Last 4-8 messages from Working Set             â”‚
â”‚  â””â”€ Active task state (if any)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ZONE 2 â€” Semantic/Procedural (40%, ~800 tokens)   â”‚
â”‚  â”œâ”€ Relevant facts (from retrieval pipeline)       â”‚
â”‚  â”œâ”€ User preferences (from retrieval pipeline)     â”‚
â”‚  â””â”€ Procedural memories (format as instructions)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ZONE 3 â€” Episodic Color (20%, ~400 tokens)        â”‚
â”‚  â”œâ”€ Related past events (only if relevant)         â”‚
â”‚  â””â”€ Can be 0 if no episodes match                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Card Format
Each retrieved memory is presented to Gemini as a compact card:

```
ðŸ“Œ [SEMANTIC] Erden's car wash business
   Erden runs a car wash business and is building a website for it using Next.js.
   He prefers modern, premium-looking designs with dark mode support.
   Strength: 0.92 | Last seen: 2 days ago

ðŸ“Œ [EPISODIC] SSL certificate debugging session (Feb 5)
   Erden was frustrated while debugging SSL certs at 2:30 AM.
   Resolved by generating a new self-signed cert with proper SAN fields.
   Strength: 0.71 | Last seen: 8 days ago

ðŸ“Œ [PROCEDURAL] Deployment preference
   Always check systemd status first. Use `journalctl -u <service> -f` for live logs.
   Strength: 0.85 | Last seen: 3 days ago
```

This format is compact (~50-80 tokens per card) and gives Gemini everything it needs to use the memory effectively, including confidence signals (strength) that help it weight information.

---

## 8. How "Scent â†’ Story" Works (Associative Recall)

**Example scenario:** Erden says "remember that keyboard issue?"

1. **Cue extraction:** keywords = ["keyboard", "issue", "remember"]
2. **Cue query:** `SELECT node_id FROM memory_cues WHERE cue_value IN ('keyboard', 'issue')` â†’ finds node #47 ("Erden mentioned wanting a mechanical keyboard") and node #82 ("Keyboard shortcut conflict in VS Code")
3. **Spreading activation:** Follow edges from #47 â†’ finds #48 ("Erden was comparing Cherry MX switches") with edge weight 0.7, and #50 ("Budget discussion â€” Erden set aside $200 for peripherals") with edge weight 0.4
4. **Result:** Even though Erden only said "keyboard issue," Aika recalls the full cluster: the keyboard desire, the switch comparison, and the budget context. Exactly like smelling cookies and remembering your grandmother's kitchen.

**Why edges decay matters here:** If Erden discussed keyboards once, 6 months ago, and never again, the edges from "keyboard" to "budget" and "switches" will have decayed to near zero. Only the core memory (#47) survives if it was strong enough. The associated details fade â€” just like a human who vaguely remembers wanting a keyboard but not the specifics.

---

## 9. Comparison: Current vs. Proposed

| Aspect | Current | Proposed |
|--------|---------|----------|
| **Storage** | 10 messages + 2 summaries | Unlimited nodes + edges + cues |
| **Retrieval** | Fixed window (last 10) | Cue-based, relevance-ranked |
| **Long-term memory** | 4-sentence global summary | Individual semantic/procedural nodes with full detail |
| **Associations** | None | Weighted edge graph with decay |
| **Forgetting** | Aggressive (1 hour expiry) | Gradual strength decay, edges decay faster |
| **Context assembly** | Dump everything | Budget-aware composer, only relevant memories |
| **Detail preservation** | Lost after summarization | Preserved in individual nodes indefinitely |
| **Cost per turn** | 1 Groq call (summary), 1 Gemini call | 0-1 Groq calls (extraction), 1 Gemini call, 0 for retrieval |
| **Recall from old context** | Impossible (summarized away) | Possible if cue matches and strength > threshold |

---

## 10. Implementation Phases

### Phase 1: Foundation (Minimal Viable Memory)
**Build these 3 things to get 80% of the effect:**

1. **Semantic memory extraction & storage**
   - On each turn, use Groq to extract facts/preferences â†’ store as nodes with cues
   - Replace global/weekly summary with actual semantic nodes
   - Deduplicate on insert (match by cues)

2. **Cue-based retrieval + Context Composer**
   - On each turn, extract keywords from user message
   - Query `memory_cues` for matches â†’ rank by strength + cue overlap
   - Assemble context with budget: working set + top semantic matches
   - Replace the current "dump summaries" approach

3. **Decay on strength + reinforcement on use**
   - Decay node strength based on time since last access
   - Reinforce when retrieved into context
   - Simple SQL UPDATE on each retrieval cycle

### Phase 2: Associations (The "Scent â†’ Story" Layer)
4. **Memory edges with spreading activation**
   - Link co-occurring nodes (same turn, shared entities)
   - 1-hop activation during retrieval
   - Edge decay (faster than node decay)

5. **Episodic memory**
   - Store notable events as timestamped scenes
   - Retrieve when user references past events or time periods

### Phase 3: Polish
6. **Procedural memory**
   - Extract behavioral patterns over time
   - Inject as system prompt modifications (not context cards)

7. **Reconsolidation**
   - Periodically merge/refine semantic nodes that are near-duplicates
   - Update facts when contradicted by newer information

8. **Raw blobs (evidence archive)**
   - Store original transcript chunks linked to nodes
   - Allows "expanding" a memory card to full detail on demand

---

## 11. Practical Considerations for Raspberry Pi 5

### Why This Works on a Pi
- **No embeddings needed for v1.** Keyword/entity matching on SQLite with proper indexes is plenty fast for a single-user system with <10,000 memories. We can add embeddings later if needed.
- **Groq does the heavy lifting.** Memory extraction, which is the most LLM-intensive part, runs on Groq's `qwen3-32b` (free tier, fast). Gemini is only used for the actual conversation.
- **SQLite is perfect.** Single-user, single-writer, lightweight. The Pi 5 has plenty of RAM for this workload.
- **Background processing.** Like the current system, all memory operations (extraction, linking, decay) run as background tasks after the reply is sent. The user never waits.

### Token Cost Analysis
- **Current system:** ~200 tokens for summaries + ~500 tokens for buffer = ~700 tokens of memory context per turn. Quality: LOW (generic summaries).
- **Proposed system:** ~400 tokens for working set + ~400-600 tokens for 6-8 memory cards = ~800-1000 tokens. Quality: HIGH (specific, relevant, ranked).
- **Net effect:** ~30-40% more tokens spent on memory, but information density per token increases by 5-10x because we inject RELEVANT memories instead of a generic summary.

### Groq Cost
- **Current:** 1 Groq call per buffer overflow (summarization).
- **Proposed:** 1 Groq call per turn (memory extraction). Slightly more calls, but each is small and fast. Groq free tier handles this easily for a single-user bot.

---

## 12. What We're NOT Doing (And Why)

| Idea | Why we skip it |
|------|---------------|
| **Vector embeddings** | Overkill for v1. Keyword matching with good cue extraction handles single-user recall well. Can add later as Phase 4. |
| **Multiple users with shared memory** | Current system is single-user. If needed later, add a `user_id` column. |
| **LLM-based retrieval ranking** | Too expensive. SQL-based scoring (cue matches + strength + recency) is fast and free. |
| **Separate embedding model on Pi** | Adds complexity and RAM pressure. Not needed until we have >10,000 memories. |
| **Real-time consolidation** | Groq extraction after each turn is enough. Periodic cleanup (hourly/daily) handles the rest. |

---

## 13. Summary: The Philosophy

> **Current Aika:** Has amnesia with a sticky note on the fridge.
>
> **Proposed Aika:** Has a mind that naturally remembers what matters, forgets what doesn't, and can be reminded of old memories by the right cue â€” just like a human brain, but with the machine advantage of never truly deleting anything strong enough to persist.

The key architectural shift is: **don't compress memories into summaries â€” extract them into structured nodes and retrieve selectively.** Summaries destroy information. Nodes preserve it. A good retriever surfaces the right nodes at the right time, while a budget composer ensures we never waste tokens on irrelevant recall.

This is not just an optimization. It's a fundamentally different relationship between storage and attention.
